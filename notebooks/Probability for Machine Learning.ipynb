{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability and Total Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the word \"secret\" in many spam emails. However, some emails are not spam even though they contain the word \"secret.\" Let's say we know the following probabilities:\n",
    "\n",
    "The probability of getting a spam email is 23.88%. That is,\n",
    "$P(Spam) = 0.2388$\n",
    "\n",
    "The probability of an email containing the word \"secret\" given that the email is spam is 48.02%. That is,\n",
    "$P(\\text{\"secret\"}| Spam) = 0.4802$\n",
    "\n",
    "The probability of an email containing the word \"secret\" given that the email is not spam is 12.84%. That is,\n",
    "$P(\\text{\"secret\"}| Spam^C) = 0.1284$\n",
    "\n",
    "Calculate:\n",
    "- $P(Spam^C)$. Assign the result to p_non_spam.\n",
    "- $P(Spam ∩ \\text{\"secret\"})$. Assign the result to p_spam_and_secret.\n",
    "- $P(Spam^C ∩ \\text{\"secret\"})$. Assign the result to p_non_spam_and_secret.\n",
    "- $P(\\text{\"secret\"})$. Assign the result to p_secret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_spam = 0.2388\n",
    "p_secret_given_spam = 0.4802\n",
    "p_secret_given_non_spam = 0.1284\n",
    "\n",
    "p_non_spam = 1 - p_spam\n",
    "p_spam_and_secret = p_spam * p_secret_given_spam\n",
    "p_non_spam_and_secret = p_non_spam * p_secret_given_non_spam\n",
    "p_secret = p_spam_and_secret + p_non_spam_and_secret\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An airline transports passengers using two types of planes: a Boeing 737 and an Airbus A320.\n",
    "\n",
    "The Boeing operates 73% of the flights. Out of these flights, 3% arrive at the destination with a delay.\n",
    "The Airbus operates the remaining 27% of the flights. Out of these flights, 8% arrive with a delay.\n",
    "Convert the percentages above to probabilities:\n",
    "\n",
    "1. Assign the probability of flying with a Boeing to p_boeing (to better understand what this probability means, imagine a passenger having bought a ticket with this airline — what's the probability that this passenger will be assigned to fly to her destination with a Boeing?).\n",
    "2. Assign the probability of flying with an Airbus to p_airbus.\n",
    "3. Assign the probability of arriving at the destination with a delay given that the passenger flies with a Boeing to p_delay_given_boeing.\n",
    "4. Assign the probability of arriving at the destination with a delay given that the passenger flies with an Airbus to p_delay_given_airbus.\n",
    "\n",
    "Calculate:\n",
    "5. The probability that a passenger will arrive at her destination with a delay. Assign your answer to p_delay. Check the hint if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_boeing = .73\n",
    "p_airbus = .27\n",
    "p_delay_given_boeing = .03\n",
    "p_delay_given_airbus = .08\n",
    "\n",
    "p_delay = p_boeing * p_delay_given_boeing + p_airbus * p_delay_given_airbus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more general formula is as given below. This formula accounts for all the events that may be present. For example, in addition to airbus and boeing, we have delay for another flight carrier named fl1, so, the below formula will be able to accommodate such an addition.\n",
    "\n",
    "$\\begin{equation}\n",
    "\\overbrace{P(A)}^{P(Delay)} = \\overbrace{P(B_1)}^{P(Boeing)} \\cdot P(A|B_1) + \\overbrace{P(B_2)}^{P(Airbus)} \\cdot P(A|B_2) + \\overbrace{P(B_3)}^{P(ERJ)} \\cdot P(A|B_3)\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An airline transports passengers using three types of planes: a Boeing 737, an Airbus A320, and an ERJ 145.\n",
    "\n",
    "The Boeing operates 62% of the flights. Out of these flights, 6% arrive at the destination with a delay.\n",
    "The Airbus operates 35% of the flights. Out of these flights, 9% arrive with a delay.\n",
    "The ERJ operates the remaining 3% of the flights. Out of these flights, 1% arrive with a delay.\n",
    "\n",
    "1. Calculate the probability of delay and assign your result to p_delay. See the hint if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_boeing = 0.62\n",
    "p_airbus = 0.35\n",
    "p_erj = 0.03\n",
    "p_delay_boeing = 0.06 \n",
    "p_delay_airbus = 0.09\n",
    "p_delay_erj = 0.01\n",
    "\n",
    "p_delay = p_boeing*p_delay_boeing + p_airbus*p_delay_airbus + p_erj*p_delay_erj\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same reasoning as we used above, the formula for n events is:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(A) = P(B_1) \\cdot P(A|B_1) + P(B_2) \\cdot P(A|B_2) + \\dots + P(B_n) \\cdot P(A|B_n)\n",
    "\\end{equation}$\n",
    "\n",
    "The above formula is called the law of total probability.\n",
    "\n",
    "$\\begin{equation}\n",
    "P(A) = \\sum_{i = 1}^{n} P(B_i) \\cdot P(A|B_i)\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On previous screens, we discussed a few examples around plane delays and tried to use the law of total probability to find P(Delay), the probability that a passenger will arrive at her destination with a delay. Once a plane arrived with a delay, however, we might be interested to calculate the probability that it's a Boeing. In other words, what's the probability that the plane is a Boeing given that it arrived with a delay?\n",
    "\n",
    "Let's bring back a concrete example we've used earlier. An airline transports passengers using two types of planes: a Boeing 737 and an Airbus A320.\n",
    "\n",
    "The Boeing operates 73% of the flights. Out of these flights, 3% arrive at the destination with a delay.\n",
    "The Airbus operates the remaining 27% of the flights. Out of these flights, 8% arrive with a delay.\n",
    "Let's say a plane did arrive with a delay and we want to find the probability that the plane is a Boeing. In other words, we want to find P(Boeing|Delay). Let's begin by expanding P(Boeing|Delay) using the conditional probability formula:\n",
    "$\\begin{equation}\n",
    "P(Boeing|Delay) = \\frac{P(Boeing \\cap Delay)}{P(Delay)} = \\frac{P(Boeing) \\cdot P(Delay|Boeing)}{P(Delay)}\n",
    "\\end{equation}$\n",
    "\n",
    "We already know from the problem statement that  and . We don't know the value of P(Delay), but we can find it using the law of total probability:\n",
    "$\\begin{aligned}\n",
    "P(Delay) &= P(Boeing) \\cdot P(Delay|Boeing) + P(Airbus) \\cdot P(Delay|Airbus) \\\\\n",
    "&= 0.73 \\cdot 0.03 + 0.27 \\cdot 0.08 = 0.0435\n",
    "\\end{aligned}$\n",
    "\n",
    "Now we can plug in the values in our initial conditional probability formula and find P(Boeing|Delay):\n",
    "$\\begin{equation}\n",
    "P(Boeing|Delay) = \\frac{0.73 \\cdot 0.03}{0.0435} = 0.5034\n",
    "\\end{equation}$\n",
    "\n",
    "This is an instance where we applied Bayes' theorem to solve a probability problem. Mathematically, Bayes' theorem can be defined as:\n",
    "$\\begin{equation}\n",
    "P(B|A) = \\frac{P(B) \\cdot P(A|B)}{\\displaystyle \\sum_{i = 1}^{n} P(B_i) \\cdot P(A|B_i)}\n",
    "\\end{equation}$\n",
    "\n",
    "Note that we arrived at Bayes' theorem by substituting the law of total probability into the conditional probability formula and expanding the numerator P(B ∩ A) using the multiplication rule:\n",
    "\\begin{aligned}\n",
    "\\text{Conditional Probability} &\\implies P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\\\\n",
    "\\text{The Law of Total Probability} &\\implies P(A) = \\sum_{i = 1}^{n} P(B_i) \\cdot P(A|B_i) \\\\\n",
    "\\text{Bayes' Theorem} &\\implies P(B|A) = \\frac{P(B) \\cdot P(A|B)}{\\displaystyle \\sum_{i = 1}^{n} P(B_i) \\cdot P(A|B_i)}\n",
    "\\end{aligned}\n",
    "\n",
    "Above, we defined the formulas for P(B|A), but we can also define them for P(A|B):\n",
    "\\begin{aligned}\n",
    "\\text{Conditional Probability} &\\implies P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\\\\n",
    "\\text{The Law of Total Probability} &\\implies P(B) = \\sum_{i = 1}^{n} P(A_i) \\cdot P(B|A_i) \\\\\n",
    "\\text{Bayes' Theorem} &\\implies P(A|B) = \\frac{P(A) \\cdot P(B|A)}{\\displaystyle \\sum_{i = 1}^{n} P(A_i) \\cdot P(B|A_i)}\n",
    "\\end{aligned}\n",
    "\n",
    "Now let's use Bayes' theorem to find P(Airbus|Delay). On the next screen, we'll learn more about Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An airline transports passengers using two types of planes: a Boeing 737 and an Airbus A320.\n",
    "\n",
    "The Boeing operates 73% of the flights. Out of these flights, 3% arrive at the destination with a delay.\n",
    "The Airbus operates the remaining 27% of the flights. Out of these flights, 8% arrive with a delay.\n",
    "1. Use Bayes' theorem to find P(Airbus|Delay). Assign your answer to p_airbus_delay. Don't forget you can check the hint if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_boeing = 0.73\n",
    "p_airbus = 0.27\n",
    "p_delay_given_boeing = 0.03\n",
    "p_delay_given_airbus = 0.08\n",
    "\n",
    "numerator = p_airbus * p_delay_given_airbus\n",
    "denominator = p_airbus*p_delay_given_airbus + p_boeing*p_delay_given_boeing\n",
    "p_airbus_delay = numerator / denominator\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of getting a positive test result given that a patient is infected with HIV is 99.78%. That is, $P(T^+ | HIV) = 0.9978$\n",
    "\n",
    "The probability of getting a positive test result given that a patient is not infected with HIV is 1.05%. That is, $P(T^+ | HIV^C) = 0.0105$\n",
    "\n",
    "The probability of being infected with HIV is 0.14%. That is, $P(HIV) = 0.0014$\n",
    "\n",
    "The probability of not being infected with HIV is 99.86%. That is, $P(HIV^C) = 0.9986$\n",
    "\n",
    "Since $P(T^+ | HIV) = 0.9978$, it means that 99.78% of the people infected with HIV get a correct diagnosis — they test positive for a virus they actually have.\n",
    "\n",
    "The value of $P(T^+ | HIV^C) = 0.0105$ means 1.05% of the persons who are not infected with HIV get a wrong diagnosis — they test positive for a virus they don't have. All in all, this suggests the test is quite efficient.\n",
    "\n",
    "Now let's say a person comes in for a test and we don't know beforehand whether they have HIV or not. The patient tests positive. One important question we may have now is: Given the positive test result, what's the probability of being infected with HIV? In other words, what is P(HIV|T+)?\n",
    "\n",
    "We can find the answer using Bayes' theorem. Let's begin by expanding P(HIV|T+) using the conditional probability formula:\n",
    "$\\begin{equation}\n",
    "P(HIV | T^+) = \\frac{P(HIV \\cap T^+)}{P(T^+)} = \\frac{P(HIV) \\cdot P(T^+|HIV)}{P(T^+)}\n",
    "\\end{equation}$\n",
    "\n",
    "From the problem statement, we know that  and . We don't know the value of P(T+), but we can find it using the law of total probability:\n",
    "$\\begin{aligned}\n",
    "P(T^+) &= P(HIV) \\cdot P(T^+|HIV) + P(HIV^C) \\cdot P(T^+|HIV^C) \\\\\n",
    "&= 0.0014 \\cdot 0.9978 + 0.9986 \\cdot 0.0105 = 0.0119\n",
    "\\end{aligned}$\n",
    "\n",
    "We now have all the values we need to calculate P(HIV|T+):\n",
    "$\\begin{equation}\n",
    "P(HIV | T^+) = \\frac{0.0014 \\cdot 0.9978}{0.0119} = 0.1174\n",
    "\\end{equation}$\n",
    "\n",
    "We see that if a person tests positive, the probability of being infected with HIV is still pretty low: 11.74%. This low value may be a bit counter-intuitive given the high efficiency of the test. However, the probability is low because P(HIV) — the probability of having HIV — is very low in the first place: 0.14%.\n",
    "\n",
    "Notice, however, that if a person tests positively, the probability of being infected with HIV actually increases a lot. The regular person in the population has a 0.14% chance to be infected with HIV — since . But if a person tests positively, the probability of HIV infection increases to 11.74%, which is about 84 times more than the initial probability!\n",
    "$\\begin{equation}\n",
    "\\frac{P(HIV|T^+)}{P(HIV)} = \\frac{0.1174}{0.0014} = 83.85\n",
    "\\end{equation}$\n",
    "\n",
    "In the above example, we've considered the probability of being infected with HIV in two scenarios:\n",
    "\n",
    "1. Before doing any test: P(HIV)\n",
    "2. After testing positive: P(HIV|T+)\n",
    "The probability of being infected with HIV before doing any test is called the prior probability (\"prior\" means \"before\"). The probability of being infected with HIV after testing positive is called the posterior probability (\"posterior\" means \"after\"). So, in this case, the prior probability is 0.14%, and the posterior probability is 11.74%.\n",
    "\n",
    "Now let's look at an exercise about spam emails and wrap up this mission on the next screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many spam emails contain the word \"secret\". However, some emails are not spam even though they contain the word \"secret\". Let's say we know the following probabilities:\n",
    "\n",
    "The probability of getting a spam email is 23.88%. That is, $P(Spam) = 0.2388$\n",
    "\n",
    "The probability of an email containing the word \"secret\" given that the email is spam is 48.02%. That is, $P(\\text{\"secret\"}| Spam) = 0.4802$\n",
    "\n",
    "The probability of an email containing the word \"secret\" given that the email is not spam is 12.84%. That is, $P(\\text{\"secret\"}| Spam^C) = 0.1284$\n",
    "\n",
    "1. Use Bayes' theorem to find P(Spam|\"secret\"). Assign your answer to p_spam_given_secret.\n",
    "\n",
    "2. Assign the prior probability of getting a spam email to prior.\n",
    "\n",
    "3. Assign the posterior probability of getting a spam email (after we see the email contains the word \"secret\") to posterior.\n",
    "\n",
    "4. Calculate the ratio between the posterior and the prior probability — you'll need to divide the posterior probability by the prior probability. Assign your answer to ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_spam = 0.2388\n",
    "p_secret_given_spam = 0.4802\n",
    "p_secret_given_non_spam = 0.1284\n",
    "\n",
    "p_non_spam = 1 - p_spam\n",
    "numerator = p_spam * p_secret_given_spam\n",
    "denominator = p_spam * p_secret_given_spam + p_non_spam * p_secret_given_non_spam\n",
    "p_spam_given_secret = numerator / denominator\n",
    "\n",
    "prior = p_spam\n",
    "posterior = p_spam_given_secret\n",
    "ratio = posterior / prior\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to classify a message as spam or non-spam\n",
    "1. The computer learns how humans classify messages.\n",
    "2. Then it uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "3. Finally, the computer classifies a new message based on the probability values it calculated in step 2 — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may want a human to classify the message — we'll come back to this issue in the guided project).\n",
    "\n",
    "When a new message comes in, the algorithm requires the computer to calculate the following probabilities:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) =\\ ? \\\\\n",
    "P(Spam^C |New\\ message) =\\ ?\n",
    "\\end{equation}$\n",
    "\n",
    "Let's take the first equation and expand it using Bayes' theorem:\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) = \\frac{P(Spam) \\cdot P(New\\ Message | Spam)}{P(New\\ message)}\n",
    "\\end{equation}$\n",
    "\n",
    "Now let's do the same for the second equation:\n",
    "$\\begin{equation}\n",
    "P(Spam^C | New\\ message) = \\frac{P(Spam^C) \\cdot P(New\\ Message | Spam^C)}{P(New\\ message)}\n",
    "\\end{equation}$\n",
    "\n",
    "For the sake of example, let's assume the following probabilities are already known:\n",
    "\n",
    "$\\begin{aligned}\n",
    "&P(Spam) = 0.5 \\\\\n",
    "&P(Spam^C) = 0.5 \\\\\n",
    "&P(New\\ message) = 0.4167 \\\\\n",
    "&P(New\\ Message | Spam) = 0.5 \\\\\n",
    "&P(New\\ Message | Spam^C) = 0.3334\n",
    "\\end{aligned}$\n",
    "\n",
    "If the computer knows these values, then it can calculate the probabilities it needs to classify a new message:\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) = \\frac{0.5 \\cdot 0.5}{0.4167} = 0.6 \\\\\n",
    "P(Spam^C | New\\ message) = \\frac{0.5 \\cdot 0.3334}{0.4167} = 0.4\n",
    "\\end{equation}$\n",
    "\n",
    "Since $P(Spam | New\\ message) > P(Spam^C | New\\ message)$, the computer will classify the new message as spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new mobile message has been received: \"URGENT!! You have one day left to claim your $873 prize.\" The following probabilities are known:\n",
    "\n",
    "$\\begin{aligned}\n",
    "&P(Spam) = 0.5 \\\\\n",
    "&P(Spam^C) = 0.5 \\\\\n",
    "&P(New\\ message) = 0.5417 \\\\\n",
    "&P(New\\ Message | Spam) = 0.75 \\\\\n",
    "&P(New\\ Message | Spam^C) = 0.3334\n",
    "\\end{aligned}$\n",
    "\n",
    "Classify this new message as spam or non-spam:\n",
    "\n",
    "Calculate P(Spam|New Message). Assign your answer to p_spam_given_new_message.\n",
    "Calculate P(SpamC|New Message). Assign your answer to p_non_spam_given_new_message.\n",
    "Classify the message by comparing the probability values. If the message is spam, then assign the string 'spam' to the variable classification. Otherwise, assign the string 'non-spam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_spam = 0.5\n",
    "p_non_spam = 0.5\n",
    "p_new_message = 0.5417\n",
    "p_new_message_given_spam = 0.75\n",
    "p_new_message_given_non_spam = 0.3334\n",
    "\n",
    "p_spam_given_new_message = (p_spam * p_new_message_given_spam) / p_new_message\n",
    "p_non_spam_given_new_message = (p_non_spam * p_new_message_given_non_spam) / p_new_message\n",
    "\n",
    "classification = 'spam' if p_spam_given_new_message > p_non_spam_given_new_message else 'non-spam'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the last screen, we saw the computer can use these two equations to calculate the probabilities it needs to classify new messages:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) = \\frac{P(Spam) \\cdot P(New\\ Message | Spam)}{P(New\\ message)} \\\\\n",
    "P(Spam^C | New\\ message) = \\frac{P(Spam^C) \\cdot P(New\\ Message | Spam^C)}{P(New\\ message)}\n",
    "\\end{equation}$\n",
    "\n",
    "Although we've taken a great first step so far, the actual equations of the Naive Bayes algorithm are a bit different — we'll gradually develop the equations throughout this mission. Let's start by pointing out that both equations above have the same denominator: P(New message).\n",
    "\n",
    "When a new message comes in, P(New message) has the same value for both equations. Since we only need to compare the results of the two equations to classify a new message, we can ignore the division:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac{P(Spam) \\cdot P(New\\ Message | Spam)}{P(New\\ message)}\\ \\ \\ \\  \\xrightarrow[]{becomes}\\ \\ \\ \\  P(Spam) \\cdot P(New\\ Message | Spam) \\\\\n",
    "\\frac{P(Spam^C) \\cdot P(New\\ Message | Spam^C)}{P(New\\ message)}\\ \\ \\  \\xrightarrow[]{becomes}\\ \\ \\ \\  P(Spam^C) \\cdot P(New\\ Message | Spam^C)\n",
    "\\end{equation}$\n",
    "\n",
    "This means our two equations reduce to:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) = P(Spam) \\cdot P(New\\ Message | Spam) \\\\\n",
    "P(Spam^C | New\\ message) = P(Spam^C) \\cdot P(New\\ Message | Spam^C)\n",
    "\\end{equation}$\n",
    "\n",
    "Ignoring the division doesn't affect the algorithm's ability to classify new messages. For instance, let's repeat the classification we did on the previous screen using the new equations above. Recall that we assumed we already know these values:\n",
    "\n",
    "$\\begin{aligned}\n",
    "&P(Spam) = 0.5 \\\\\n",
    "&P(Spam^C) = 0.5 \\\\\n",
    "&P(New\\ message) = 0.4167 \\\\\n",
    "&P(New\\ Message | Spam) = 0.5 \\\\\n",
    "&P(New\\ Message | Spam^C) = 0.3334\n",
    "\\end{aligned}$\n",
    "\n",
    "Previously, the algorithm classified the new message as spam. Using the new equations, we see the conclusion is identical — the new message is spam because $P(Spam | New\\ message) > P(Spam^C | New\\ message)$:\n",
    "\n",
    "$\\begin{aligned}\n",
    "P(Spam^C | New\\ message) &= P(Spam^C) \\cdot P(New\\ Message | Spam^C) \\\\\n",
    "&= 0.5 \\cdot 0.3334 = 0.1667\n",
    "\\end{aligned}$\n",
    "\n",
    "The classification works fine, but ignoring the division changes the probability values, and some probability rules also begin to break. For instance, let's take this conditional probability rule that we've learned about in a previous mission:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(A|B) + P(A^C|B) = 1\n",
    "\\end{equation}$\n",
    "\n",
    "On the previous screen, we saw $P(Spam | New\\ message) = 0.6$ and $P(Spam^C | New\\ message) = 0.4$, and the rule holds with these values:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) + P(Spam^C | New\\ message) = 0.6 + 0.4 = 1\n",
    "\\end{equation}$\n",
    "\n",
    "With the values we got from the new equations, however, the law breaks:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) + P(Spam^C | New\\ message) = 0.25 + 0.1667 = 0.4167 \\not = 1\n",
    "\\end{equation}$\n",
    "\n",
    "Even though probability rules break, the Naive Bayes algorithm still requires us to ignore the division by P(New message). This might not make a lot of sense, but there's actually a very good reason we do that.\n",
    "\n",
    "The main goal of the algorithm is to classify new messages, not to calculate probabilities — calculating probabilities is just a means to an end. Ignoring the division by P(New message) means less calculations, which can make a lot of difference when we use the algorithm to classify 500,000 new messages.\n",
    "\n",
    "It's true the probability values are not accurate anymore. However, this is not important with respect to the the goal of the algorithm — correctly classifying new messages (not to accurately estimate probabilities).\n",
    "\n",
    "The classification itself remains completely unaffected because we ignore division for both equations (not just for one). The probability values change, but they change directly proportional with one another, so the result of the comparison doesn't change.\n",
    "\n",
    "For instance, $\\frac{8}{4} > \\frac{4}{4}$. If we ignore the division, the values change directly proportional with respect to one another such that the result of the comparison stays the same: $8 > 4$.\n",
    "\n",
    "The symbol for directly proportional is $\\propto$, and it's more accurate to replace the equality sign with  in our two equations:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) \\propto P(Spam) \\cdot P(New\\ Message | Spam) \\\\\n",
    "P(Spam^C | New\\ message) \\propto P(Spam^C) \\cdot P(New\\ Message | Spam^C)\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new mobile message has been received: \"URGENT!! You have one day left to claim your $873 prize.\" The following probabilities are known:\n",
    "\n",
    "$\\begin{aligned}\n",
    "&P(Spam) = 0.5 \\\\\n",
    "&P(Spam^C) = 0.5 \\\\\n",
    "&P(New\\ Message | Spam) = 0.75 \\\\\n",
    "&P(New\\ Message | Spam^C) = 0.3334\n",
    "\\end{aligned}$\n",
    "\n",
    "Use the new equations we learned on this screen, and classify the new message as spam or non-spam:\n",
    "\n",
    "1. Calculate P(Spam|New Message). Assign your answer to p_spam_given_new_message.\n",
    "2. Calculate P(SpamC|New Message). Assign your answer to p_non_spam_given_new_message.\n",
    "3. Classify the message by comparing the probability values — if the message is spam, then assign the string 'spam' to the variable classification. Otherwise, assign the string 'non-spam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_spam = 0.5\n",
    "p_non_spam = 0.5\n",
    "p_new_message_given_spam = 0.75\n",
    "p_new_message_given_non_spam = 0.3334\n",
    "\n",
    "p_spam_given_new_message = p_spam * p_new_message_given_spam\n",
    "p_non_spam_given_new_message = p_non_spam * p_new_message_given_non_spam\n",
    "\n",
    "classification = 'spam' if p_spam_given_new_message > p_non_spam_given_new_message else 'non-spam'\n",
    "```"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous screen, we optimized the algorithm and concluded that we can use these two optimized equations if all we're interested in is classifying messages (and not calculating accurate probabilities):\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | New\\ message) \\propto P(Spam) \\cdot P(New\\ Message | Spam) \\\\\n",
    "P(Spam^C | New\\ message) \\propto P(Spam^C) \\cdot P(New\\ Message | Spam^C)\n",
    "\\end{equation}$\n",
    "\n",
    "We'll now look at how the algorithm can use messages that are already classified by humans to calculate the values it needs for:\n",
    "\n",
    "- P(Spam) and P(SpamC)\n",
    "- P(New message|Spam) and P(New message|SpamC).\n",
    "\n",
    "We'll start with some examples that may look a bit too simplistic and unrealistic, but they will make it easier to understand the mathematics behind the algorithm.\n",
    "\n",
    "Let's say we have three messages that are already classified:\n",
    "\n",
    "|ix| Label   | SMS   |\n",
    "|--|---------|-------|\n",
    "|0 | spam    | secret money secret secret |\n",
    "|1 | spam    | money secret place |\n",
    "|2 | non-spam| you know the secret |\n",
    "\n",
    "Now let's say the one-word message \"secret\" comes in and we want to use the Naive Bayes algorithm to classify it — to tell whether it's spam or non-spam.\n",
    "\n",
    "|ix| Label   | SMS   |\n",
    "|--|---------|-------|\n",
    "|0 | spam    | secret money secret secret |\n",
    "|1 | spam    | money secret place |\n",
    "|2 | non-spam| you know the secret |\n",
    "|__3__|__?__|__secret__ |\n",
    "\n",
    "As we learned, we first need to answer these two probability questions (note that we changed New Message to \"secret\" inside the notation below) and then compare the values (recall that the $\\propto$ symbol replaces the equal sign):\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | \\text{\"secret\"}) \\propto P(Spam) \\cdot P(\\text{\"secret\"} | Spam) \\\\\n",
    "P(Spam^C | \\text{\"secret\"}) \\propto P(Spam^C) \\cdot P(\\text{\"secret\"} | Spam^C)\n",
    "\\end{equation}$\n",
    "\n",
    "Let's begin with the first equation, for which we need to find the values of P(Spam) and P(\"secret\"|Spam). To find P(Spam), we use the messages that are already classified and divide the number of spam messages by the total number of messages:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam) = \\frac{\\text{number of spam messages}}{\\text{total number of messages}} = \\frac{2}{3}\n",
    "\\end{equation}$\n",
    "\n",
    "To calculate P(\"secret\"|Spam), we only look at the spam messages and divide the number of times the word \"secret\" occurred in all the spam messages by the total number of words.\n",
    "\n",
    "$\\begin{equation}\n",
    "P(\\text{\"secret\"}| Spam) = \\frac{\\text{number of times the word \"secret\" occurs}}{\\text{total number of words in all spam messages}}\n",
    "\\end{equation}$\n",
    "\n",
    "Notice that \"secret\" occurs four times in the spam messages:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We have two spam messages and there's a total of seven words in all of them, so P(\"secret\"|Spam) is:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(\\text{\"secret\"}| Spam) = \\frac{\\text{number of times the word \"secret\" occurs}}{\\text{total number of words in all spam messages}} = \\frac{4}{7}\n",
    "\\end{equation}$\n",
    "\n",
    "Now that we know the values for P(Spam) and P(\"secret\"|Spam), we have all we need to calculate P(Spam|\"secret\"):\n",
    "\n",
    "$\\begin{aligned}\n",
    "P(Spam | \\text{\"secret\"}) &\\propto P(Spam) \\cdot P(\\text{\"secret\"} | Spam) \\\\\n",
    "&= \\frac{2}{3} \\cdot \\frac{4}{7} = \\frac{8}{21}\n",
    "\\end{aligned}$\n",
    "\n",
    "For the exercise below, we'll take the same steps as above to calculate P(SpamC|\"secret\"). Then, we can compare the values of P(SpamC|\"secret\") and P(Spam|\"secret\") to classify the message \"secret\" as spam or non-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate P(SpamC) and assign the answer to p_non_spam.\n",
    "- Calculate P(\"secret\"|SpamC) and assign the answer to p_secret_given_non_spam.\n",
    "- Calculate P(SpamC|\"secret\") and assign the answer to p_non_spam_given_secret.\n",
    "- Compare P(SpamC|\"secret\") with P(Spam|\"secret\") and classify the message \"secret\" — if the message is spam, then assign the string 'spam' to the variable classification, otherwise assign the string 'non-spam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_spam_given_secret = 8/21\n",
    "\n",
    "p_non_spam = 1/3\n",
    "p_secret_given_non_spam = 1/4\n",
    "p_non_spam_given_secret = p_non_spam*p_secret_given_non_spam\n",
    "\n",
    "classification = 'spam' if p_spam_given_secret > p_non_spam_given_secret else 'non-spam'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous screen, we used our algorithm to classify the message \"secret\", and we concluded it's spam. The message \"secret\" has only one word, but what about the situation where we have to classify messages that have more words?\n",
    "\n",
    "Let's say we want to classify the message \"secret place secret secret\" based on four messages that are already classified (the four messages below are different than what what we saw on the previous screen):\n",
    "\n",
    "![](image1.png)\n",
    "\n",
    "To calculate the probabilities we need, we'll treat each word in our new message separately. This means that the word \"secrete\" at the beginning is different and separate from the word \"secret\" at the end. There are four words in the message \"secret place secret secret\", and we're going to abbreviate them \"w1\", \"w2\", \"w3\" and \"w4\" (the \"w\" comes from \"word\").\n",
    "\n",
    "![](image2.png)\n",
    "\n",
    "Since we treat each word separately, these are the two equations we can use to calculate the probabilities:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2,w_3,w_4) \\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot P(w_3|Spam) \\cdot P(w_4|Spam) \\\\\n",
    "P(Spam^C | w_1,w_2,w_3,w_4) \\propto P(Spam^C) \\cdot P(w_1|Spam^C) \\cdot P(w_2|Spam^C) \\cdot P(w_3|Spam^C) \\cdot P(w_4|Spam^C) \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "Let's begin with calculating P(Spam|w1, w2, w3, w4). To calculate the probabilities we need, we'll look at the four messages that are already classified. We have four messages and two of them are spam, so:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam) = \\frac{2}{4} = \\frac{1}{2}\n",
    "\\end{equation}$\n",
    "\n",
    "The first word, w1, is \"secret\", and we see that \"secret\" occurs four times in all spam messages. There's a total of seven words in all the spam messages, so:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(w_1|Spam) = \\frac{4}{7} \n",
    "\\end{equation}$\n",
    "\n",
    "Applying a similar reasoning, we have:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(w_2|Spam) = \\frac{1}{7} \\\\\n",
    "P(w_3|Spam) = \\frac{4}{7} \\\\\n",
    "P(w_4|Spam) = \\frac{4}{7}\n",
    "\\end{equation}$\n",
    "\n",
    "We now have all the probabilities we need to calculate P(Spam|w1, w2, w3, w4):\n",
    "\n",
    "$\\begin{aligned}\n",
    "P(Spam | w_1,w_2,w_3,w_4) &\\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot P(w_3|Spam) \\cdot P(w_4|Spam) \\\\\n",
    "&= \\frac{1}{2} \\cdot \\frac{4}{7} \\cdot \\frac{1}{7} \\cdot \\frac{4}{7} \\cdot \\frac{4}{7} = \\frac{64}{4802} = 0.01333\n",
    "\\end{aligned}$\n",
    "\n",
    "Let's now take similar steps to calculate P(SpamC|w1, w2, w3, w4), and then classify the message \"secret place secret secret\" as spam or non-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate P(SpamC|w1, w2, w3, w4). Assign the answer to p_non_spam_given_w1_w2_w3_w4. Check the hint if you get stuck.\n",
    "- Compare P(SpamC|w1, w2, w3, w4) with P(Spam|w1, w2, w3, w4) and classify the message \"secret place secret secret\" — if the message is spam, then assign the string 'spam' to the variable classification. Otherwise, assign the string 'non-spam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "p_spam_given_w1_w2_w3_w4 = 64/4802\n",
    "\n",
    "p_non_spam = 2/4\n",
    "p_w1_given_non_spam = 2/9\n",
    "p_w2_given_non_spam = 1/9\n",
    "p_w3_given_non_spam = 2/9\n",
    "p_w4_given_non_spam = 2/9\n",
    "\n",
    "p_non_spam_given_w1_w2_w3_w4 = (p_non_spam *\n",
    "                                p_w1_given_non_spam * p_w2_given_non_spam *\n",
    "                                p_w3_given_non_spam * p_w4_given_non_spam\n",
    "                               )\n",
    "\n",
    "classification = 'spam' if p_spam_given_w1_w2_w3_w4 > p_non_spam_given_w1_w2_w3_w4 else 'non-spam'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous screen, we introduced these two equations without much explanation:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2,w_3,w_4) \\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot P(w_3|Spam) \\cdot P(w_4|Spam) \\\\\n",
    "P(Spam^C | w_1,w_2,w_3,w_4) \\propto P(Spam^C) \\cdot P(w_1|Spam^C) \\cdot P(w_2|Spam^C) \\cdot P(w_3|Spam^C) \\cdot P(w_4|Spam^C) \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "To explain the mathematics behind these equations, let's start by looking at P(Spam|w1, w2, w3, w4). Using the conditional probability formula, we can expand P(Spam|w1, w2, w3, w4) like this (below, make sure you notice the $\\cap$ symbol in the numerator):\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2,w_3,w_4) = \\frac{P(Spam \\cap (w_1, w_2, w_3, w_4))}{P(w_1, w_2, w_3, w_4)}\n",
    "\\end{equation}$\n",
    "\n",
    "Recall that we learned in a previous screen that we can ignore the division, which means we can drop P(w1, w2, w3, w4) to avoid redundant calculations (when we ignore the division, we also replace the equals sign with $\\propto$, which means directly proportional):\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2,w_3,w_4) \\propto P(Spam \\cap (w_1, w_2, w_3, w_4))\n",
    "\\end{equation}$\n",
    "\n",
    "Note that (w1, w2, w3, w4) can be modeled as an intersection of four events:\n",
    "\n",
    "$\\begin{equation}\n",
    "w_1,w_2,w_3,w_4 = w_1 \\cap w_2 \\cap w_3 \\cap w_4\n",
    "\\end{equation}$\n",
    "\n",
    "For instance, we could think of a message like \"thanks for your help\" as the intersection of four words inside a single message: \"thanks\", \"for, \"your\", and \"help\". In probability jargon, finding the value of $P(w_1 \\cap w_2 \\cap w_3 \\cap w_4)$ means finding the probability that the four words w1, w2, w3, w4 occur together in a single message — this is similar to $P(A \\cap B \\cap C \\cap D)$, which is the probability that events A, B, C, and D occur together.\n",
    "\n",
    "With this in mind, our equation above transforms to:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2,w_3,w_4) \\propto P(Spam \\cap \\underbrace{(w_1 \\cap w_2 \\cap w_3 \\cap w_4)}_{\\displaystyle (w_1,w_2,w_3,w_4)})\n",
    "\\end{equation}$\n",
    "\n",
    "From set theory, we know that $A \\cap (B \\cap C) = A \\cap B \\cap C = C \\cap B \\cap A$, which means we can transform $P(Spam \\cap (w_1 \\cap w_2 \\cap w_3 \\cap w_4))$ in our equation above to make it suitable for further expansion:\n",
    "\n",
    "$\\begin{aligned}\n",
    "P(Spam \\cap (w_1 \\cap w_2 \\cap w_3 \\cap w_4)) &= P(Spam \\cap w_1 \\cap w_2 \\cap w_3 \\cap w_4) \\\\\n",
    "&= P(w_1 \\cap w_2 \\cap w_3 \\cap w_4 \\cap Spam)\n",
    "\\end{aligned}$\n",
    "\n",
    "Now let's use the multiplication rule to expand $P(w_1 \\cap w_2 \\cap w_3 \\cap w_4 \\cap Spam)$:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(w_1 \\cap w_2 \\cap w_3 \\cap w_4 \\cap Spam) = P(w_1 | w_2 \\cap w_3 \\cap w_4 \\cap Spam) \\cdot P(w_2 \\cap w_3 \\cap w_4 \\cap Spam)\n",
    "\\end{equation}$\n",
    "\n",
    "We can use the multiplication rule again to expand P(w2 ∩ w3 ∩ w4 ∩ Spam):\n",
    "\n",
    "$\\begin{equation}\n",
    "P(w_1 \\cap w_2 \\cap w_3 \\cap w_4 \\cap Spam) = P(w_1 | w_2 \\cap w_3 \\cap w_4 \\cap Spam) \\cdot \\underbrace{P(w_2 | w_3 \\cap w_4 \\cap Spam) \\cdot P(w_3 \\cap w_4 \\cap Spam)}_{\\displaystyle P(w_2 \\cap w_3 \\cap w_4 \\cap Spam)}\n",
    "\\end{equation}$\n",
    "\n",
    "We can use the multiplication rule successively, until there's nothing more left to expand:\n",
    "\n",
    "$\\begin{aligned}\n",
    "P(w_1 \\cap w_2 \\cap w_3 \\cap w_4 \\cap Spam) &= P(w_1 | w_2 \\cap w_3 \\cap w_4 \\cap Spam) \\cdot P(w_2 \\cap w_3 \\cap w_4 \\cap Spam) \\\\\n",
    "&= P(w_1 | w_2 \\cap w_3 \\cap w_4 \\cap Spam) \\cdot P(w_2 | w_3 \\cap w_4 \\cap Spam) \\cdot P(w_3 \\cap w_4 \\cap Spam) \\\\\n",
    "&= P(w_1 | w_2 \\cap w_3 \\cap w_4 \\cap Spam) \\cdot P(w_2 | w_3 \\cap w_4 \\cap Spam) \\cdot P(w_3 | w_4 \\cap Spam) \\cdot P(w_4 \\cap Spam) \\\\\n",
    "&= P(w_1 | w_2 \\cap w_3 \\cap w_4 \\cap Spam) \\cdot P(w_2 | w_3 \\cap w_4 \\cap Spam) \\cdot P(w_3 | w_4 \\cap Spam) \\cdot P(w_4|Spam) \\cdot P(Spam) \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "In theory, the last equation you see above is what we'd have to use if we wanted to calculate P(Spam|w1, w2, w3, w4). However, the equation is pretty long for just four words. Also, imagine how would the equation look for a 50-word message — just think of how many calculations we'd have to perform!!\n",
    "\n",
    "To make the calculations tractable for messages of all kinds of lengths, we can assume conditional independence between w1, w2, w3, and w4. This implies that:\n",
    "\n",
    "$\\begin{aligned}\n",
    "&P(w_1 | w_2 \\cap w_3 \\cap w_4 \\cap Spam) = P(w_1|Spam) \\\\\n",
    "&P(w_2 | w_3 \\cap w_4 \\cap Spam) = P(w_2|Spam) \\\\ \n",
    "&P(w_3 | w_4 \\cap Spam) = P(w_3|Spam) \\\\\n",
    "&P(w_4|Spam) = P(w_4|Spam) \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "Under the assumption of independence, our lengthy equation above reduces to:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(w_1 \\cap w_2 \\cap w_3 \\cap w_4 \\cap Spam) = P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot P(w_3|Spam) \\cdot P(w_4|Spam) \\cdot P(Spam)\n",
    "\\end{equation}$\n",
    "\n",
    "The assumption of conditional independence is unrealistic in practice because words are often in a relationship of dependence. For instance, if you see the word \"WINNER\" in a message, the probability of seeing the word \"money\" is very likely to increase, so \"WINNER\" and \"money\" are most likely dependent. The assumption of conditional independence between words is thus naive since it rarely holds in practice, and this is why the algorithm is called Naive Bayes (also called simple Bayes or independence Bayes).\n",
    "\n",
    "Despite this simplifying assumption, the algorithm works quite well in many real-word situations, and we'll see that ourselves in the guided project.\n",
    "\n",
    "That being said, on the previous screen we assumed conditional independence when we introduced these two equations:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2,w_3,w_4) \\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot P(w_3|Spam) \\cdot P(w_4|Spam) \\\\\n",
    "P(Spam^C | w_1,w_2,w_3,w_4) \\propto P(Spam^C) \\cdot P(w_1|Spam^C) \\cdot P(w_2|Spam^C) \\cdot P(w_3|Spam^C) \\cdot P(w_4|Spam^C) \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "On the next screen, we'll make the equations more general to account for any number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous screen, we learned about the conditional independence assumption, which is central to the Naive Bayes algorithm. As a result of the assumption, we saw we can use these simplified equations:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2,w_3,w_4) \\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot P(w_3|Spam) \\cdot P(w_4|Spam) \\\\\n",
    "P(Spam^C | w_1,w_2,w_3,w_4) \\propto P(Spam^C) \\cdot P(w_1|Spam^C) \\cdot P(w_2|Spam^C) \\cdot P(w_3|Spam^C) \\cdot P(w_4|Spam^C) \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "The equations above work for messages that have four words, but we need a more general form to use with messages of various word lengths.\n",
    "\n",
    "A new message has n words, where n can be any positive integer (1, 2, 3, ..., 50, 51, 53, ...). If we wanted to find P(Spam|w1, w2, ..., wn), then this is an equation we could use:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2, \\ldots, w_n) \\propto P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot \\ldots \\cdot P(w_n|Spam)\n",
    "\\end{equation}$\n",
    "\n",
    "Notice that there's a certain pattern in the equation above — after P(Spam), the only thing that changes is the word number.\n",
    "\n",
    "Whenever we have a product that follows a pattern like that, it's common to use the  symbol (this is the uppercase Greek letter \"pi\"). So the equation above simplifies to:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2, \\ldots, w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}$\n",
    "\n",
    "The equation above is the same as:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2, \\ldots, w_n) \\propto P(Spam) \\cdot \\overbrace{P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot \\ldots \\cdot P(w_n|Spam)}^{\\displaystyle \\prod_{i=1}^{n}P(w_i|Spam)}\n",
    "\\end{equation}$\n",
    "\n",
    "Applying the same reasoning to P(SpamC|w1, w2, ..., wn), we have:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam^C | w_1,w_2, \\ldots, w_n) \\propto P(Spam^C) \\cdot \\prod_{i=1}^{n}P(w_i|Spam^C)\n",
    "\\end{equation}$\n",
    "\n",
    "Now that we have these general equations, we're going to discuss a few edge cases on the next few screens. After this, we'll be ready to start working on the guided project, where we'll work with over 5,000 real messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a previous screen, we looked at a few messages that were already classified:\n",
    "\n",
    "![image3.png](image3.png)\n",
    "\n",
    "Above, we have four messages and nine unique words: \"secret\", \"party\", \"at\", \"my\", \"place\", \"money\", \"you\", \"know\", \"the\". We call the set of unique words a vocabulary.\n",
    "\n",
    "Now, what if we receive a new message that contains words which are not part of the vocabulary? How do we calculate probabilities for this kind of words?\n",
    "\n",
    "For instance, say we received the message \"secret code to unlock the money\".\n",
    "\n",
    "![image4.png](image4.png)\n",
    "\n",
    "Notice that for this new message:\n",
    "\n",
    "- The words \"code\", \"to\", and \"unlock\" are not part of the vocabulary.\n",
    "- The word \"secret\" is part of both spam and non-spam messages.\n",
    "- The word \"money\" is only part of the spam messages and is missing from the non-spam messages.\n",
    "- The word \"the\" is missing from the spam messages and is only part of the non-spam messages.\n",
    "\n",
    "Whenever we have to deal with words that are not part of the vocabulary, one solution is to ignore them when we're calculating probabilities. If we wanted to calculate P(Spam|\"secret code to unlock the money\"), we could skip calculating P(\"code\"|Spam), P(\"to\"|Spam), and P(\"unlock\"|Spam) because \"code\", \"to\", and \"unlock\" are not part of the vocabulary:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam|\\text{\"secret code to unlock the money\"}) \\propto P(Spam) \\cdot {P(\\text{\"secret\"}|Spam) \\cdot P(\\text{\"the\"}|Spam) \\cdot P(\\text{\"money\"}|Spam)}\n",
    "\\end{equation}$\n",
    "\n",
    "We can also apply the same reasoning for calculating P(SpamC|\"secret code to unlock the money\"):\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam^C|\\text{\"secret code to unlock the money\"}) \\propto P(Spam^C) \\cdot P(\\text{\"secret\"}|Spam^C) \\cdot P(\\text{\"the\"}|Spam^C) \\cdot P(\\text{\"money\"}|Spam^C)\n",
    "\\end{equation}$\n",
    "\n",
    "Let's now calculate P(Spam|\"secret code to unlock the money\") and P(SpamC|\"secret code to unlock the money\"), and see what we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Spam|\"secret code to unlock the money\") is already calculated for you. Use the table below (the same as above) to calculate P(SpamC|\"secret code to unlock the money\").\n",
    "\n",
    "![](image5.png)\n",
    "\n",
    "Calculate P(SpamC|\"secret code to unlock the money\"). Assign your answer to p_non_spam_given_message.\n",
    "Print p_spam_given_message and p_non_spam_given_message. Why do you think we got these values? We'll discuss more about this in the next screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "p_spam = 2/4\n",
    "p_secret_given_spam = 4/7\n",
    "p_the_given_spam = 0/7\n",
    "p_money_given_spam = 2/7\n",
    "p_spam_given_message = (p_spam * p_secret_given_spam *\n",
    "                        p_the_given_spam * p_money_given_spam)\n",
    "\n",
    "p_non_spam = 2/4\n",
    "p_secret_given_non_spam = 2/9\n",
    "p_the_given_non_spam = 1/9\n",
    "p_money_given_non_spam = 0/9\n",
    "p_non_spam_given_message = (p_non_spam * p_secret_given_non_spam *\n",
    "                        p_the_given_non_spam * p_money_given_non_spam)\n",
    "\n",
    "print(p_spam_given_message)\n",
    "print(p_non_spam_given_message)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, we saw that both P(Spam|\"secret code to unlock the money\") and P(SpamC|\"secret code to unlock the money\") were equal to 0. This will always happen when we have words that occur in only one category — \"money\" occurs only in spam messages, while \"the\" only occurs in non-spam messages.\n",
    "\n",
    "![](image6.png)\n",
    "\n",
    "When we calculate P(Spam|\"secret code to unlock the money\"), we can see that P(\"the\"|Spam) is equal to 0 because \"the\" is not part of the spam messages. Unfortunately, that single value of 0 has the drawback of turning the result of the entire equation to 0:\n",
    "\n",
    "$\\begin{aligned}\n",
    "P(Spam|\\text{\"secret code to unlock the money\"}) &\\propto P(Spam) \\cdot P(\\text{\"secret\"}|Spam) \\cdot P(\\text{\"the\"}|Spam) \\cdot P(\\text{\"money\"}|Spam) \\\\\n",
    "&= \\frac{2}{4} \\cdot \\frac{4}{7} \\cdot \\frac{0}{7} \\cdot \\frac{2}{7} = 0\n",
    "\\end{aligned}$\n",
    "\n",
    "To fix this problem, we need to find a way to avoid these cases where we get probabilities of 0. Let's start by laying out the equation we're using to calculate P(\"the\"|Spam):\n",
    "\n",
    "$\\begin{equation}\n",
    "P(\\text{\"the\"}|Spam) = \\frac{\\text{total number of times \"the\" occurs in spam messages}}{\\text{total number of words in spam messages}} = \\frac{0}{7}\n",
    "\\end{equation}$\n",
    "\n",
    "We're going to add some notation and rewrite the equation above as:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(\\text{\"the\"}|Spam) = \\frac{N_{\\text{\"the\"}|Spam}}{N_{Spam}} = \\frac{0}{7}\n",
    "\\end{equation}$\n",
    "\n",
    "To fix the problem, we're going to use a technique called `additive smoothing`, where we add a smoothing parameter $\\alpha$. In the equation below, we'll use $\\alpha = 1$ (below, NVocabulary represents the number of unique words in all the messages — both spam and non-spam).\n",
    "\n",
    "$\\begin{equation}\n",
    "P(\\text{\"the\"}|Spam) = \\frac{N_{\\text{\"the\"}|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} = \\frac{0 + 1}{7 + 1 \\cdot 9} = \\frac{1}{16}\n",
    "\\end{equation}$\n",
    "\n",
    "The additive smoothing technique solves the issue and gets us a non-zero result, but it introduces another problem. We're now calculating probabilities differently depending on the word — take P(\"the\"|Spam) and P(\"secret\"|Spam) for instance:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(\\text{\"the\"}|Spam) = \\frac{N_{\\text{\"the\"}|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(\\text{\"secret\"}|Spam) = \\frac{N_{\\text{\"secret\"}|Spam}}{N_{Spam}}\n",
    "\\end{equation}$\n",
    "\n",
    "Words like \"the\" are thus given special treatment and their probability are increased artificially to avoid non-zero cases, while words like \"secret\" are treated normally. To keep the probability values proportional across all words, we're going to use the additive smoothing for every word:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(\\text{\"the\"}|Spam) = \\frac{N_{\\text{\"the\"}|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(\\text{\"secret\"}|Spam) = \\frac{N_{\\text{\"secret\"}|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}$\n",
    "\n",
    "In more general terms, this is the equation that we'll need to use for every word:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(word|Spam) = \\frac{N_{\\text{word}|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}$\n",
    "\n",
    "As a side note, when $\\alpha = 1$, the additive smoothing technique is most commonly known as Laplace smoothing (or add-one smoothing). However, it is also possible to use $\\alpha < 1$, in which case the technique is called Lidstone smoothing. If you want to learn more about additive smoothing, you can start [here](https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "\n",
    "Let's now recalculate the probabilities for the message \"secret code to unlock the money\" and try to classify the message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Spam|\"secret code to unlock the money\") is already calculated for you. Use the table below (the same as above) to calculate P(SpamC|\"secret code to unlock the money\").\n",
    "\n",
    "![](image7.png)\n",
    "\n",
    "- Using the additive smoothing technique, calculate P(SpamC|\"secret code to unlock the money\"). Assign your answer to p_non_spam_given_message.\n",
    "- Compare p_spam_given_message and p_non_spam_given_message to classify the message as spam or non-spam. If you think it's spam, then assign the string 'spam' to classification. Otherwise, assign 'non-spam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "p_spam = 2/4\n",
    "p_secret_given_spam = (4 + 1) / (7 + 9)\n",
    "p_the_given_spam = (0 + 1) / (7 + 9)\n",
    "p_money_given_spam = (2 + 1) / (7 + 9)\n",
    "p_spam_given_message = (p_spam * p_secret_given_spam *\n",
    "                        p_the_given_spam * p_money_given_spam)\n",
    "\n",
    "p_non_spam = 2/4\n",
    "p_secret_given_non_spam = (2 + 1)/(9 + 9)\n",
    "p_the_given_non_spam = (1 + 1)/(9 + 9)\n",
    "p_money_given_non_spam = (0 + 1)/(9 + 9)\n",
    "p_non_spam_given_message = (p_non_spam * p_secret_given_non_spam *\n",
    "                        p_the_given_non_spam * p_money_given_non_spam) \n",
    "\n",
    "classification = 'spam' if p_spam_given_message > p_non_spam_given_message else 'non-spam'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes algorithm can be used for more than just building spam filters. For instance, we could use it to perform sentiment analysis for Twitter messages — the input is a Twitter message, and the output is the sentiment type (positive or negative). This follows the same pattern we saw with our spam filter, where the input is a new SMS message and the output is the message type (spam or non-spam).\n",
    "\n",
    "Depending on the math and the assumptions used, the Naive Bayes algorithm has a few variations. The three most popular Naive Bayes algorithms are:\n",
    "\n",
    "- Multinomial Naive Bayes\n",
    "- Gaussian Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "\n",
    "In this mission, we learned the multinomial Naive Bayes version of the algorithm. Explaining the mathematical differences between the various versions is out of the scope of this course, but it's important to keep in mind that all the Naive Bayes algorithms build on the (naive) conditional independence assumption we learned about earlier in this mission.\n",
    "\n",
    "On the next screen, we'll summarize everything we've done so far, and then we'll wrap up this mission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize everything we've done so far, these are the two equations we can use for our spam filtering problem moving forward:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}$\n",
    "\n",
    "$\\begin{equation}\n",
    "P(Spam^C | w_1,w_2, ..., w_n) \\propto P(Spam^C) \\cdot \\prod_{i=1}^{n}P(w_i|Spam^C)\n",
    "\\end{equation}$\n",
    "\n",
    "To calculate P(wi|Spam) and P(wi|SpamC), we need to use the additive smoothing technique:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}$\n",
    "\n",
    "$\\begin{equation}\n",
    "P(w_i|Spam^C) = \\frac{N_{w_i|Spam^C} + \\alpha}{N_{Spam^C} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}$\n",
    "\n",
    "Let's also summarize what the terms in the equations above mean:\n",
    "\n",
    "$\\begin{aligned}\n",
    "&N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages} \\\\\n",
    "&N_{w_i|Spam^C} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Spam} = \\text{total number of words in spam messages} \\\\\n",
    "&N_{Spam^C} = \\text{total number of words in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Vocabulary} = \\text{total number of words in the vocabulary} \\\\\n",
    "&\\alpha = 1 \\ \\ \\ \\ (\\alpha \\text{ is a smoothing parameter})\n",
    "\\end{aligned}$\n",
    "\n",
    "It's worth emphasizing that:\n",
    "\n",
    "- NSpam is equal to the number of words in all the spam messages — it's not equal to the number of spam messages, and it's not equal to the total number of unique words in spam messages.\n",
    "- NSpamC is equal to the number of words in all the non-spam messages — it's not equal to the number of non-spam messages, and it's not equal to the total number of unique words in non-spam messages.\n",
    "\n",
    "In the next mission, which is a guided project, we'll use the multinomial Naive Bayes algorithm to create a spam filter, and we'll use a dataset of over 5,000 SMS messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
